{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv1yUcSgd27m"
      },
      "source": [
        "# Exploring the latent space of stable diffusion\n",
        "\n",
        "What's interesting is that you can generate an image from any point in latent space.\n",
        "\n",
        "## Outline\n",
        "\n",
        "1.   Simple way to run different neural networks for different tasks\n",
        "  1.   Huggingface pipelines\n",
        "  2.   Huggingface models\n",
        "  3.   Specific libraries\n",
        "3.   Creating a GUI and automating computer vision\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEhtH959DiOC"
      },
      "source": [
        "First, let's install all the required modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T17:20:28.329767Z",
          "iopub.status.busy": "2024-02-21T17:20:28.329050Z",
          "iopub.status.idle": "2024-02-21T17:23:15.653382Z",
          "shell.execute_reply": "2024-02-21T17:23:15.652310Z",
          "shell.execute_reply.started": "2024-02-21T17:20:28.329734Z"
        },
        "id": "lbWtDpayDiOD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install -q diffusers transformers xformers accelerate\n",
        "!pip install -q numpy scipy ftfy Pillow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUUXab_IDiOE"
      },
      "source": [
        "Import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T17:23:55.606390Z",
          "iopub.status.busy": "2024-02-21T17:23:55.606005Z",
          "iopub.status.idle": "2024-02-21T17:24:12.144679Z",
          "shell.execute_reply": "2024-02-21T17:24:12.143740Z",
          "shell.execute_reply.started": "2024-02-21T17:23:55.606352Z"
        },
        "id": "gbnW1HiEDiOE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import time\n",
        "\n",
        "from PIL import Image\n",
        "from IPython import display as IPdisplay\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from diffusers import (\n",
        "    DDIMScheduler,\n",
        "    PNDMScheduler,\n",
        "    LMSDiscreteScheduler,\n",
        "    DPMSolverMultistepScheduler,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    EulerDiscreteScheduler,\n",
        ")\n",
        "from transformers import logging\n",
        "\n",
        "logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loFaaWVUDiOF"
      },
      "source": [
        "Let's check if CUDA is available.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T17:24:16.252373Z",
          "iopub.status.busy": "2024-02-21T17:24:16.251653Z",
          "iopub.status.idle": "2024-02-21T17:24:16.258088Z",
          "shell.execute_reply": "2024-02-21T17:24:16.257085Z",
          "shell.execute_reply.started": "2024-02-21T17:24:16.252340Z"
        },
        "id": "uGgmrhr-DiOF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(torch.cuda.is_available())\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMSGnuqmDiOF"
      },
      "source": [
        "These settings are used to optimize the performance of PyTorch models on CUDA-enabled GPUs, especially when using mixed precision training or inference, which can be beneficial in terms of speed and memory usage.       \n",
        "Source: https://huggingface.co/docs/diffusers/optimization/fp16#memory-efficient-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T17:24:18.661531Z",
          "iopub.status.busy": "2024-02-21T17:24:18.661171Z",
          "iopub.status.idle": "2024-02-21T17:24:18.666289Z",
          "shell.execute_reply": "2024-02-21T17:24:18.665171Z",
          "shell.execute_reply.started": "2024-02-21T17:24:18.661501Z"
        },
        "id": "JT02KQqNDiOF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E5R20VtDiOF"
      },
      "source": [
        "### Model\n",
        "\n",
        "The [`stabilityai/stable-diffusion-2-1-base`](https://huggingface.co/stabilityai/stable-diffusion-2-1-base) model and the EulerDiscreteSchedulerscheduler were chosen to generate images. Despite being an older technology, it continues to enjoy popularity due to its fast performance, minimal memory requirements, and the availability of numerous community fine-tuned models built on top of SD1.5. However, you are free to experiment with other models and schedulers to compare the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T17:24:22.143953Z",
          "iopub.status.busy": "2024-02-21T17:24:22.143589Z",
          "iopub.status.idle": "2024-02-21T17:25:54.037631Z",
          "shell.execute_reply": "2024-02-21T17:25:54.036655Z",
          "shell.execute_reply.started": "2024-02-21T17:24:22.143923Z"
        },
        "id": "ppKz1aLSDiOF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model_name_or_path = \"stabilityai/stable-diffusion-2-1-base\"\n",
        "\n",
        "scheduler = EulerDiscreteScheduler.from_pretrained(model_name_or_path, subfolder=\"scheduler\")\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    scheduler=scheduler,\n",
        "    torch_dtype=torch.float32,\n",
        ").to(device)\n",
        "\n",
        "# Disable image generation progress bar, we'll display our own\n",
        "pipe.set_progress_bar_config(disable=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oBmcxe9DiOG"
      },
      "source": [
        "These methods are designed to reduce the memory consumed by the GPU. If you have enough VRAM, you can skip this cell.   \n",
        "\n",
        "More detailed information can be found here: https://huggingface.co/docs/diffusers/en/optimization/opt_overview     \n",
        "In particular, information about the following methods can be found here: https://huggingface.co/docs/diffusers/optimization/memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T17:25:54.040235Z",
          "iopub.status.busy": "2024-02-21T17:25:54.039388Z",
          "iopub.status.idle": "2024-02-21T17:26:00.115879Z",
          "shell.execute_reply": "2024-02-21T17:26:00.115042Z",
          "shell.execute_reply.started": "2024-02-21T17:25:54.040193Z"
        },
        "id": "1i7WuQV1DiOG",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# # Offloading the weights to the CPU and only loading them on the GPU can reduce memory consumption to less than 3GB.\n",
        "# pipe.enable_model_cpu_offload()\n",
        "\n",
        "# # Tighter ordering of memory tensors.\n",
        "# pipe.unet.to(memory_format=torch.channels_last)\n",
        "\n",
        "# # Decoding large batches of images with limited VRAM or batches with 32 images or more by decoding the batches of latents one image at a time.\n",
        "# pipe.enable_vae_slicing()\n",
        "\n",
        "# # Splitting the image into overlapping tiles, decoding the tiles, and then blending the outputs together to compose the final image.\n",
        "# pipe.enable_vae_tiling()\n",
        "\n",
        "# # Using Flash Attention; If you have PyTorch >= 2.0 installed, you should not expect a speed-up for inference when enabling xformers.\n",
        "# pipe.enable_xformers_memory_efficient_attention()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k45VkXF7DiOG"
      },
      "source": [
        "The `display_images` function converts a list of image arrays into a GIF, saves it to a specified path and returns the GIF object for display. It names the GIF file using the current time and handles any errors by printing them out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T17:30:01.535670Z",
          "iopub.status.busy": "2024-02-21T17:30:01.535281Z",
          "iopub.status.idle": "2024-02-21T17:30:01.542928Z",
          "shell.execute_reply": "2024-02-21T17:30:01.541894Z",
          "shell.execute_reply.started": "2024-02-21T17:30:01.535637Z"
        },
        "id": "n5cKlS0CDiOG",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def display_images(images, save_path):\n",
        "    try:\n",
        "        # Convert each image in the 'images' list from an array to an Image object.\n",
        "        images = [\n",
        "            Image.fromarray(np.array(image[0], dtype=np.uint8)) for image in images\n",
        "        ]\n",
        "\n",
        "        # Generate a file name based on the current time, replacing colons with hyphens\n",
        "        # to ensure the filename is valid for file systems that don't allow colons.\n",
        "        filename = (\n",
        "            time.strftime(\"%H:%M:%S\", time.localtime())\n",
        "            .replace(\":\", \"-\")\n",
        "        )\n",
        "        # Save the first image in the list as a GIF file at the 'save_path' location.\n",
        "        # The rest of the images in the list are added as subsequent frames to the GIF.\n",
        "        # The GIF will play each frame for 100 milliseconds and will loop indefinitely.\n",
        "        images[0].save(\n",
        "            f\"{save_path}/{filename}.gif\",\n",
        "            save_all=True,\n",
        "            append_images=images[1:],\n",
        "            duration=100,\n",
        "            loop=0,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        # If there is an error during the process, print the exception message.\n",
        "        print(e)\n",
        "\n",
        "    # Return the saved GIF as an IPython display object so it can be displayed in a notebook.\n",
        "    return IPdisplay.Image(f\"{save_path}/{filename}.gif\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L13Q7INNDiOG"
      },
      "source": [
        "### Generation parameters\n",
        "\n",
        "\n",
        "* `seed`: This variable is used to set a specific random seed for reproducibility.      \n",
        "* `generator`: This is set to a PyTorch random number generator object if a seed is provided, otherwise it is None. It ensures that the operations using it have reproducible outcomes.     \n",
        "* `guidance_scale`: This parameter controls the extent to which the model should follow the prompt in text-to-image generation tasks, with higher values leading to stronger adherence to the prompt.       \n",
        "* `num_inference_steps`: This specifies the number of steps the model takes to generate an image. More steps can lead to a higher quality image but take longer to generate.        \n",
        "* `num_interpolation_steps`: This determines the number of steps used when interpolating between two points in the latent space, affecting the smoothness of transitions in generated       animations.        \n",
        "* `height`: The height of the generated images in pixels.       \n",
        "* `width`: The width of the generated images in pixels.     \n",
        "* `save_path`: The file system path where the generated gifs will be saved.       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T17:30:04.013629Z",
          "iopub.status.busy": "2024-02-21T17:30:04.012881Z",
          "iopub.status.idle": "2024-02-21T17:30:04.019551Z",
          "shell.execute_reply": "2024-02-21T17:30:04.018612Z",
          "shell.execute_reply.started": "2024-02-21T17:30:04.013596Z"
        },
        "id": "R_B-h2j4DiOG",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# The seed is set to \"None\", because we want different results each time we run the generation.\n",
        "seed = None\n",
        "\n",
        "if seed is not None:\n",
        "    generator = torch.manual_seed(seed)\n",
        "else:\n",
        "    generator = None\n",
        "\n",
        "# The guidance scale is set to its normal range (7 - 10).\n",
        "guidance_scale = 8\n",
        "\n",
        "# The number of inference steps was chosen empirically to generate an acceptable picture within an acceptable time.\n",
        "num_inference_steps = 20\n",
        "\n",
        "# The higher you set this value, the smoother the interpolations will be. However, the generation time will increase. This value was chosen empirically.\n",
        "num_interpolation_steps = 10\n",
        "\n",
        "# I would not recommend less than 512 on either dimension. This is because this model was trained on 512x512 image resolution.\n",
        "height = 512\n",
        "width = 512\n",
        "\n",
        "# The path where the generated GIFs will be saved\n",
        "save_path = \"/output\"\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm4BHESjDiOG"
      },
      "source": [
        "### Example 1: Prompt interpolation\n",
        "\n",
        "In this example, interpolation between positive and negative prompt embeddings allows exploration of space between two conceptual points defined by prompts, potentially leading to variety of images blending characteristics dictated by prompts gradually. In this case, interpolation involves adding scaled deltas to original embeddings, creating a series of new embeddings that will be used later to generate images with smooth transitions between different states based on the original prompt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5bJKGULCAzc"
      },
      "source": [
        "![Example 1](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/sd_interpolation_1.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CSNrZJACAzc"
      },
      "source": [
        "First of all, we need to tokenize and obtain embeddings for both positive and negative text prompts. The positive prompt guides the image generation towards the desired characteristics, while the negative prompt steers it away from unwanted features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-21T17:40:07.727796Z",
          "iopub.status.busy": "2024-02-21T17:40:07.727407Z",
          "iopub.status.idle": "2024-02-21T17:43:50.624205Z",
          "shell.execute_reply": "2024-02-21T17:43:50.622571Z",
          "shell.execute_reply.started": "2024-02-21T17:40:07.727768Z"
        },
        "id": "YVNrz60MDiOH",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# The text prompt that describes the desired output image.\n",
        "prompt = \"Epic shot of Sweden, ultra detailed lake with an ren dear, nostalgic vintage, ultra cozy and inviting, wonderful light atmosphere, fairy, little photorealistic, digital painting, sharp focus, ultra cozy and inviting, wish to be there. very detailed, arty, should rank high on youtube for a dream trip.\"\n",
        "# A negative prompt that can be used to steer the generation away from certain features; here, it is empty.\n",
        "negative_prompt = \"poorly drawn,cartoon, 2d, disfigured, bad art, deformed, poorly drawn, extra limbs, close up, b&w, weird colors, blurry\"\n",
        "\n",
        "# The step size for the interpolation in the latent space.\n",
        "step_size = 0.001\n",
        "\n",
        "# Tokenizing and encoding the prompt into embeddings.\n",
        "prompt_tokens = pipe.tokenizer(\n",
        "    prompt,\n",
        "    padding=\"max_length\",\n",
        "    max_length=pipe.tokenizer.model_max_length,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "prompt_embeds = pipe.text_encoder(prompt_tokens.input_ids.to(device))[0]\n",
        "\n",
        "\n",
        "# Tokenizing and encoding the negative prompt into embeddings.\n",
        "if negative_prompt is None:\n",
        "    negative_prompt = [\"\"]\n",
        "\n",
        "negative_prompt_tokens = pipe.tokenizer(\n",
        "    negative_prompt,\n",
        "    padding=\"max_length\",\n",
        "    max_length=pipe.tokenizer.model_max_length,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "negative_prompt_embeds = pipe.text_encoder(negative_prompt_tokens.input_ids.to(device))[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iINthMinCAzc"
      },
      "source": [
        "Now let's look at the code part that generates a random initial vector using a normal distribution that is structured to match the dimensions expected by the diffusion model (UNet). This allows for the reproducibility of the results by optionally using a random number generator. After creating the initial vector, the code performs a series of interpolations between the two embeddings (positive and negative prompts), by incrementally adding a small step size for each iteration. The results are stored in a list named \"walked_embeddings\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iB8zm0eFCAzc"
      },
      "outputs": [],
      "source": [
        "# Generating initial latent vectors from a random normal distribution, with the option to use a generator for reproducibility.\n",
        "latents = torch.randn(\n",
        "    (1, pipe.unet.config.in_channels, height // 8, width // 8),\n",
        "    generator=generator,\n",
        ")\n",
        "\n",
        "walked_embeddings = []\n",
        "\n",
        "# Interpolating between embeddings for the given number of interpolation steps.\n",
        "for i in range(num_interpolation_steps):\n",
        "    walked_embeddings.append(\n",
        "        [prompt_embeds + step_size * i, negative_prompt_embeds + step_size * i]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fmd-txBHCAzc"
      },
      "source": [
        "Finally, let's generate a series of images based on interpolated embeddings and then displaying these images. We'll iterate over an array of embeddings, using each to generate an image with specified characteristics like height, width, and other parameters relevant to image generation. Then we'll collect these images into a list. Once generation is complete we'll call the `display_image` function to save and display these images as GIF at a given save path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TC_6JNIACAzc"
      },
      "outputs": [],
      "source": [
        "# Generating images using the interpolated embeddings.\n",
        "images = []\n",
        "for latent in tqdm(walked_embeddings):\n",
        "    images.append(\n",
        "        pipe(\n",
        "            height=height,\n",
        "            width=width,\n",
        "            num_images_per_prompt=1,\n",
        "            prompt_embeds=latent[0],\n",
        "            negative_prompt_embeds=latent[1],\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            generator=generator,\n",
        "            latents=latents,\n",
        "        ).images\n",
        "    )\n",
        "\n",
        "# Display of saved generated images.\n",
        "display_images(images, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZQWop9nDiOH"
      },
      "source": [
        "### Example 2: Diffusion latents interpolation for a single prompt\n",
        "Unlike the first example, in this one, we are performing interpolation between the two embeddings of the diffusion model itself, not the prompts. Please note that in this case, we use the slerp function for interpolation. However, there is nothing stopping us from adding a constant value to one embedding instead.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-tYO2rMCAzd"
      },
      "source": [
        "![Example 2](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/sd_interpolation_2.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiW6SlhXDiOH"
      },
      "source": [
        "The function presented below stands for Spherical Linear Interpolation. It is a method of interpolation on the surface of a sphere. This function is commonly used in computer graphics to animate rotations in a smooth manner and can also be used to interpolate between high-dimensional data points in machine learning, such as latent vectors used in generative models.       \n",
        "\n",
        "The source is from Andrej Karpathy's gist: https://gist.github.com/karpathy/00103b0037c5aaea32fe1da1af553355.        \n",
        "A more detailed explanation of this method can be found at: https://en.wikipedia.org/wiki/Slerp."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grgP7UNpDiOH"
      },
      "outputs": [],
      "source": [
        "def slerp(v0, v1, num, t0=0, t1=1):\n",
        "    v0 = v0.detach().cpu().numpy()\n",
        "    v1 = v1.detach().cpu().numpy()\n",
        "\n",
        "    def interpolation(t, v0, v1, DOT_THRESHOLD=0.9995):\n",
        "        \"\"\"helper function to spherically interpolate two arrays v1 v2\"\"\"\n",
        "        dot = np.sum(v0 * v1 / (np.linalg.norm(v0) * np.linalg.norm(v1)))\n",
        "        if np.abs(dot) > DOT_THRESHOLD:\n",
        "            v2 = (1 - t) * v0 + t * v1\n",
        "        else:\n",
        "            theta_0 = np.arccos(dot)\n",
        "            sin_theta_0 = np.sin(theta_0)\n",
        "            theta_t = theta_0 * t\n",
        "            sin_theta_t = np.sin(theta_t)\n",
        "            s0 = np.sin(theta_0 - theta_t) / sin_theta_0\n",
        "            s1 = sin_theta_t / sin_theta_0\n",
        "            v2 = s0 * v0 + s1 * v1\n",
        "        return v2\n",
        "\n",
        "    t = np.linspace(t0, t1, num)\n",
        "\n",
        "    v3 = torch.tensor(np.array([interpolation(t[i], v0, v1) for i in range(num)]))\n",
        "\n",
        "    return v3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIU-nxTcDiOH"
      },
      "outputs": [],
      "source": [
        "# The text prompt that describes the desired output image.\n",
        "prompt = \"Sci-fi digital painting of an alien landscape with otherworldly plants, strange creatures, and distant planets.\"\n",
        "# A negative prompt that can be used to steer the generation away from certain features.\n",
        "negative_prompt = \"poorly drawn,cartoon, 3d, disfigured, bad art, deformed, poorly drawn, extra limbs, close up, b&w, weird colors, blurry\"\n",
        "\n",
        "# Generating initial latent vectors from a random normal distribution. In this example two latent vectors are generated, which will serve as start and end points for the interpolation.\n",
        "# These vectors are shaped to fit the input requirements of the diffusion model's U-Net architecture.\n",
        "latents = torch.randn(\n",
        "    (2, pipe.unet.config.in_channels, height // 8, width // 8),\n",
        "    generator=generator,\n",
        ")\n",
        "\n",
        "# Getting our latent embeddings\n",
        "interpolated_latents = slerp(latents[0], latents[1], num_interpolation_steps)\n",
        "\n",
        "# Generating images using the interpolated embeddings.\n",
        "images = []\n",
        "for latent_vector in tqdm(interpolated_latents):\n",
        "    images.append(\n",
        "        pipe(\n",
        "            prompt,\n",
        "            height=height,\n",
        "            width=width,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_images_per_prompt=1,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            generator=generator,\n",
        "            latents=latent_vector[None, ...],\n",
        "        ).images\n",
        "    )\n",
        "\n",
        "# Display of saved generated images.\n",
        "display_images(images, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTFrAlwrDiOI"
      },
      "source": [
        "### Example 3: Interpolation between multiple prompts\n",
        "\n",
        "In contrast to the first example, where we moved away from a single prompt, in this example, we will be interpolating between any number of prompts. To do so, we will take consecutive pairs of prompts and create smooth transitions between them. Then, we will combine the interpolations of these consecutive pairs, and instruct the model to generate images based on them. For interpolation we will use the slerp function, as in the second example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QFtzMlOCAzd"
      },
      "source": [
        "![Example 3](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/sd_interpolation_3.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGMilOrSCAzd"
      },
      "source": [
        "Once again, let's tokenize and obtain embeddings but this time for multiple positive and negative text prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_NEQjG_CAze"
      },
      "outputs": [],
      "source": [
        "# Text prompts that describes the desired output image.\n",
        "prompts = [\n",
        "    \"A cute dog in a beautiful field of lavander colorful flowers everywhere, perfect lighting, leica summicron 35mm f2.0, kodak portra 400, film grain\",\n",
        "    \"A cute cat in a beautiful field of lavander colorful flowers everywhere, perfect lighting, leica summicron 35mm f2.0, kodak portra 400, film grain\",\n",
        "]\n",
        "# Negative prompts that can be used to steer the generation away from certain features.\n",
        "negative_prompts = [\n",
        "    \"poorly drawn,cartoon, 2d, sketch, cartoon, drawing, anime, disfigured, bad art, deformed, poorly drawn, extra limbs, close up, b&w, weird colors, blurry\",\n",
        "    \"poorly drawn,cartoon, 2d, sketch, cartoon, drawing, anime, disfigured, bad art, deformed, poorly drawn, extra limbs, close up, b&w, weird colors, blurry\",\n",
        "]\n",
        "\n",
        "# NOTE: The number of prompts must match the number of negative prompts\n",
        "\n",
        "batch_size = len(prompts)\n",
        "\n",
        "# Tokenizing and encoding prompts into embeddings.\n",
        "prompts_tokens = pipe.tokenizer(\n",
        "    prompts,\n",
        "    padding=\"max_length\",\n",
        "    max_length=pipe.tokenizer.model_max_length,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "prompts_embeds = pipe.text_encoder(\n",
        "    prompts_tokens.input_ids.to(device)\n",
        ")[0]\n",
        "\n",
        "# Tokenizing and encoding negative prompts into embeddings.\n",
        "if negative_prompts is None:\n",
        "    negative_prompts = [\"\"] * batch_size\n",
        "\n",
        "negative_prompts_tokens = pipe.tokenizer(\n",
        "    negative_prompts,\n",
        "    padding=\"max_length\",\n",
        "    max_length=pipe.tokenizer.model_max_length,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "negative_prompts_embeds = pipe.text_encoder(\n",
        "    negative_prompts_tokens.input_ids.to(device)\n",
        ")[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfyyoVWmCAze"
      },
      "source": [
        "As stated earlier, we will take consecutive pairs of prompts and create smooth transitions between them with `slerp` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfUbS8w5DiOI"
      },
      "outputs": [],
      "source": [
        "# Generating initial U-Net latent vectors from a random normal distribution.\n",
        "latents = torch.randn(\n",
        "    (1, pipe.unet.config.in_channels, height // 8, width // 8),\n",
        "    generator=generator,\n",
        ")\n",
        "\n",
        "# Interpolating between embeddings pairs for the given number of interpolation steps.\n",
        "interpolated_prompt_embeds = []\n",
        "interpolated_negative_prompts_embeds = []\n",
        "for i in range(batch_size - 1):\n",
        "    interpolated_prompt_embeds.append(\n",
        "        slerp(\n",
        "            prompts_embeds[i],\n",
        "            prompts_embeds[i + 1],\n",
        "            num_interpolation_steps\n",
        "        )\n",
        "    )\n",
        "    interpolated_negative_prompts_embeds.append(\n",
        "        slerp(\n",
        "            negative_prompts_embeds[i],\n",
        "            negative_prompts_embeds[i + 1],\n",
        "            num_interpolation_steps,\n",
        "        )\n",
        "    )\n",
        "\n",
        "interpolated_prompt_embeds = torch.cat(\n",
        "    interpolated_prompt_embeds, dim=0\n",
        ").to(device)\n",
        "\n",
        "interpolated_negative_prompts_embeds = torch.cat(\n",
        "    interpolated_negative_prompts_embeds, dim=0\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JizTHXaPCAze"
      },
      "source": [
        "Finally, we need to generate images based on the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c03dG0lBCAze"
      },
      "outputs": [],
      "source": [
        "# Generating images using the interpolated embeddings.\n",
        "images = []\n",
        "for prompt_embeds, negative_prompt_embeds in tqdm(\n",
        "    zip(interpolated_prompt_embeds, interpolated_negative_prompts_embeds),\n",
        "    total=len(interpolated_prompt_embeds),\n",
        "):\n",
        "    images.append(\n",
        "        pipe(\n",
        "            height=height,\n",
        "            width=width,\n",
        "            num_images_per_prompt=1,\n",
        "            prompt_embeds=prompt_embeds[None, ...],\n",
        "            negative_prompt_embeds=negative_prompt_embeds[None, ...],\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            generator=generator,\n",
        "            latents=latents,\n",
        "        ).images\n",
        "    )\n",
        "\n",
        "# Display of saved generated images.\n",
        "display_images(images, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQqANSP2DiOI"
      },
      "source": [
        "### Example 4: Circular walk through the diffusion latent space for a single prompt\n",
        "\n",
        "This example was taken from: https://keras.io/examples/generative/random_walks_with_stable_diffusion/       \n",
        "\n",
        "Let's imagine that we have two noise components, which we'll call x and y. We start by moving from 0 to 2π and at each step we add the cosine of x and the sine of y to the result. Using this approach, at the end of our movement we end up with the same noise values ​​that we started with. This means that vectors end up turning into themselves, ending our movement.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn70u0UqCAzi"
      },
      "source": [
        "![Example 4](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/sd_interpolation_4.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac-68CTWDiOJ"
      },
      "outputs": [],
      "source": [
        "# The text prompt that describes the desired output image.\n",
        "prompt = \"Beautiful sea sunset, warm light, Aivazovsky style\"\n",
        "# A negative prompt that can be used to steer the generation away from certain features\n",
        "negative_prompt = \"picture frames\"\n",
        "\n",
        "# Generating initial latent vectors from a random normal distribution to create a loop interpolation between them.\n",
        "latents = torch.randn(\n",
        "    (2, 1, pipe.unet.config.in_channels, height // 8, width // 8),\n",
        "    generator=generator,\n",
        ")\n",
        "\n",
        "\n",
        "# Calculation of looped embeddings\n",
        "walk_noise_x = latents[0].to(device)\n",
        "walk_noise_y = latents[1].to(device)\n",
        "\n",
        "# Walking on a trigonometric circle\n",
        "walk_scale_x = torch.cos(torch.linspace(0, 2, num_interpolation_steps) * np.pi).to(\n",
        "    device\n",
        ")\n",
        "walk_scale_y = torch.sin(torch.linspace(0, 2, num_interpolation_steps) * np.pi).to(\n",
        "    device\n",
        ")\n",
        "\n",
        "# Applying interpolation to noise\n",
        "noise_x = torch.tensordot(walk_scale_x, walk_noise_x, dims=0)\n",
        "noise_y = torch.tensordot(walk_scale_y, walk_noise_y, dims=0)\n",
        "\n",
        "circular_latents = noise_x + noise_y\n",
        "\n",
        "# Generating images using the interpolated embeddings.\n",
        "images = []\n",
        "for latent_vector in tqdm(circular_latents):\n",
        "    images.append(\n",
        "        pipe(\n",
        "            prompt,\n",
        "            height=height,\n",
        "            width=width,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_images_per_prompt=1,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            generator=generator,\n",
        "            latents=latent_vector,\n",
        "        ).images\n",
        "    )\n",
        "\n",
        "# Display of saved generated images.\n",
        "display_images(images, save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQnbnOokDiOJ"
      },
      "source": [
        "## Next Steps       \n",
        "Moving forward, you can explore various parameters such as guidance scale, seed, and number of interpolation steps to observe how they affect the generated images. Additionally, consider trying out different prompts and schedulers to further enhance your results. Another valuable step would be to implement linear interpolation (`linspace`) instead of spherical linear interpolation (`slerp`) and compare the results to gain deeper insights into the interpolation process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tgbkOxbkf8B"
      },
      "source": [
        "# Finalizing\n",
        "\n",
        "When you finish working you have to remember to **stop the runtime**, because there is a time limit and to avoid wasting resources. To stop the runtime click Manage Sessions on the Runtime menu. Once the dialog opens click terminate on the current runtime.\n",
        "\n",
        "> But when you stop the runtime everything you have not saved is ⚠ **lost** ⚠, so be sure to **download** everything you want to keep before stopping it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNi8VV-wJDFi"
      },
      "source": [
        "# Credits\n",
        "\n",
        "Copy of https://huggingface.co/learn/cookbook/en/stable_diffusion_interpolation slightly modified by Taller Estampa https://tallerestampa.com / https://github.com/estampa"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30648,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}