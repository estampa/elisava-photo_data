{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv1yUcSgd27m"
      },
      "source": [
        "# Exploring the latent space of stable diffusion\n",
        "\n",
        "What's interesting is that you can generate an image from any point in latent space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbWtDpayDiOD",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # First, let's install all the required modules.\n",
        "\n",
        "!pip install -q diffusers transformers accelerate\n",
        "!pip install -q numpy scipy ftfy Pillow moviepy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gbnW1HiEDiOE"
      },
      "outputs": [],
      "source": [
        "#@markdown # Import modules\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import time\n",
        "\n",
        "from moviepy.editor import ImageSequenceClip\n",
        "from PIL import Image\n",
        "from IPython import display as IPdisplay\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from diffusers import (\n",
        "    DDIMScheduler,\n",
        "    PNDMScheduler,\n",
        "    LMSDiscreteScheduler,\n",
        "    DPMSolverMultistepScheduler,\n",
        "    EulerAncestralDiscreteScheduler,\n",
        "    EulerDiscreteScheduler,\n",
        "    AutoencoderKL,\n",
        ")\n",
        "from transformers import logging\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab import output\n",
        "import io\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "from ipywidgets import Button\n",
        "import shutil\n",
        "\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "#Let's check if CUDA is available.\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# These settings are used to optimize the performance of PyTorch models on CUDA-enabled GPUs,\n",
        "# especially when using mixed precision training or inference, which can be beneficial in terms of speed and memory usage.\n",
        "# Source: https://huggingface.co/docs/diffusers/optimization/fp16#memory-efficient-attention\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ppKz1aLSDiOF"
      },
      "outputs": [],
      "source": [
        "#@markdown # Model\n",
        "\n",
        "#@markdown The [`stabilityai/stable-diffusion-2-1-base`](https://huggingface.co/stabilityai/stable-diffusion-2-1-base) model and the EulerDiscreteSchedulerscheduler were chosen to generate images. Despite being an older technology, it continues to enjoy popularity due to its fast performance, minimal memory requirements, and the availability of numerous community fine-tuned models built on top of SD1.5. However, you are free to experiment with other models and schedulers to compare the results.\n",
        "\n",
        "\n",
        "model_name_or_path = \"stabilityai/stable-diffusion-2-1-base\"\n",
        "\n",
        "scheduler = EulerDiscreteScheduler.from_pretrained(model_name_or_path, subfolder=\"scheduler\")\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    scheduler=scheduler,\n",
        "    torch_dtype=torch.float32,\n",
        ").to(device)\n",
        "\n",
        "# Disable image generation progress bar, we'll display our own\n",
        "pipe.set_progress_bar_config(disable=True)\n",
        "\n",
        "vae = AutoencoderKL.from_pretrained(model_name_or_path, subfolder=\"vae\", torch_dtype=torch.float16).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1i7WuQV1DiOG"
      },
      "outputs": [],
      "source": [
        "#@markdown # Reduce the memory consumed by the GPU.\n",
        "\n",
        "#@markdown More detailed information can be found here: https://huggingface.co/docs/diffusers/en/optimization/opt_overview\n",
        "#@markdown In particular, information about the following methods can be found here: https://huggingface.co/docs/diffusers/optimization/memory\n",
        "\n",
        "\n",
        "# Offloading the weights to the CPU and only loading them on the GPU can reduce memory consumption to less than 3GB.\n",
        "pipe.enable_model_cpu_offload()\n",
        "\n",
        "# Tighter ordering of memory tensors.\n",
        "pipe.unet.to(memory_format=torch.channels_last)\n",
        "\n",
        "# Decoding large batches of images with limited VRAM or batches with 32 images or more by decoding the batches of latents one image at a time.\n",
        "pipe.enable_vae_slicing()\n",
        "\n",
        "# Splitting the image into overlapping tiles, decoding the tiles, and then blending the outputs together to compose the final image.\n",
        "pipe.enable_vae_tiling()\n",
        "\n",
        "# Using Flash Attention; If you have PyTorch >= 2.0 installed, you should not expect a speed-up for inference when enabling xformers.\n",
        "# pipe.enable_xformers_memory_efficient_attention()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5cKlS0CDiOG",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # Define functions\n",
        "\n",
        "# The path where the generated GIFs will be saved\n",
        "save_path = \"/content/output\"\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "# Saves the images and a GIF\n",
        "def save_images(images, output_path):\n",
        "  # Generate a file name based on the current time, replacing colons with hyphens\n",
        "    # to ensure the filename is valid for file systems that don't allow colons.\n",
        "    dirname = (\n",
        "        time.strftime(\"%H:%M:%S\", time.localtime())\n",
        "        .replace(\":\", \"-\")\n",
        "    )\n",
        "\n",
        "    save_path = os.path.join(output_path, dirname)\n",
        "\n",
        "    if not os.path.exists(save_path):\n",
        "      os.makedirs(save_path)\n",
        "\n",
        "    # Convert each image in the 'images' list from an array to an Image object.\n",
        "    converted = []\n",
        "\n",
        "    image_files = []\n",
        "    for i, image in enumerate(images):\n",
        "      image_file = f\"{save_path}/{i:03d}.png\"\n",
        "      image_files.append(image_file)\n",
        "\n",
        "      pil_image = Image.fromarray(np.array(image[0], dtype=np.uint8))\n",
        "      pil_image.save(image_file)\n",
        "      converted.append(pil_image)\n",
        "\n",
        "    clip = ImageSequenceClip(image_files, fps=10)\n",
        "    clip.write_videofile(f\"{save_path}/video.mp4\")\n",
        "\n",
        "    # Save the first image in the list as a GIF file at the 'save_path' location.\n",
        "    # The rest of the images in the list are added as subsequent frames to the GIF.\n",
        "    # The GIF will play each frame for 100 milliseconds and will loop indefinitely.\n",
        "    converted[0].save(\n",
        "        f\"{save_path}/preview.gif\",\n",
        "        save_all=True,\n",
        "        append_images=converted[1:],\n",
        "        duration=100,\n",
        "        loop=0,\n",
        "    )\n",
        "\n",
        "    return save_path\n",
        "\n",
        "# Displays the GIF saved in a path\n",
        "def display_gif(path):\n",
        "    # Return the saved GIF as an IPython display object so it can be displayed in a notebook.\n",
        "\n",
        "    gif = IPdisplay.Image(f\"{path}/preview.gif\")\n",
        "\n",
        "    def download(b):\n",
        "      shutil.make_archive(path, 'zip', path)\n",
        "      files.download(path + '.zip')\n",
        "\n",
        "    button = Button(description=\"Download Images\")\n",
        "    button.on_click(download)\n",
        "\n",
        "    display(gif, button)\n",
        "\n",
        "    # return VBox([gif, button])\n",
        "\n",
        "\n",
        "# based on https://towardsdatascience.com/stable-diffusion-using-hugging-face-501d8dbdd8\n",
        "def pil_to_latents(image):\n",
        "    '''\n",
        "    Function to convert image to latents\n",
        "    '''\n",
        "    init_image = transforms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n",
        "    init_image = init_image.to(device=device, dtype=torch.float16)\n",
        "    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215\n",
        "    return init_latent_dist\n",
        "\n",
        "def latents_to_images(latents):\n",
        "    '''\n",
        "    Function to convert latents to images\n",
        "    '''\n",
        "    latents = (1 / 0.18215) * latents\n",
        "    with torch.no_grad():\n",
        "        image = vae.decode(latents).sample\n",
        "\n",
        "    image = (image / 2 + 0.5).clamp(0, 1)\n",
        "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "    images = (image * 255).round().astype(\"uint8\")\n",
        "    # images = ([image] for image in images)\n",
        "    return images\n",
        "\n",
        "def latents_to_pil(latents):\n",
        "    '''\n",
        "    Function to convert latents to images\n",
        "    '''\n",
        "    images = latents_to_images(latents)\n",
        "    pil_images = [Image.fromarray(image[0]) for image in images]\n",
        "    return pil_images\n",
        "\n",
        "# The function presented below stands for Spherical Linear Interpolation. It is a method\n",
        "# of interpolation on the surface of a sphere. This function is commonly used in computer\n",
        "# graphics to animate rotations in a smooth manner and can also be used to interpolate\n",
        "# between high-dimensional data points in machine learning, such as latent vectors used\n",
        "# in generative models.\n",
        "\n",
        "# The source is from Andrej Karpathy's gist: https://gist.github.com/karpathy/00103b0037c5aaea32fe1da1af553355.\n",
        "# A more detailed explanation of this method can be found at: https://en.wikipedia.org/wiki/Slerp.\n",
        "\n",
        "def slerp(v0, v1, num, t0=0, t1=1):\n",
        "    v0 = v0.detach().cpu().numpy()\n",
        "    v1 = v1.detach().cpu().numpy()\n",
        "\n",
        "    def interpolation(t, v0, v1, DOT_THRESHOLD=0.9995):\n",
        "        \"\"\"helper function to spherically interpolate two arrays v1 v2\"\"\"\n",
        "        dot = np.sum(v0 * v1 / (np.linalg.norm(v0) * np.linalg.norm(v1)))\n",
        "        if np.abs(dot) > DOT_THRESHOLD:\n",
        "            v2 = (1 - t) * v0 + t * v1\n",
        "        else:\n",
        "            theta_0 = np.arccos(dot)\n",
        "            sin_theta_0 = np.sin(theta_0)\n",
        "            theta_t = theta_0 * t\n",
        "            sin_theta_t = np.sin(theta_t)\n",
        "            s0 = np.sin(theta_0 - theta_t) / sin_theta_0\n",
        "            s1 = sin_theta_t / sin_theta_0\n",
        "            v2 = s0 * v0 + s1 * v1\n",
        "        return v2\n",
        "\n",
        "    t = np.linspace(t0, t1, num)\n",
        "\n",
        "    v3 = torch.tensor(np.array([interpolation(t[i], v0, v1) for i in range(num)]))\n",
        "\n",
        "    return v3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L13Q7INNDiOG"
      },
      "source": [
        "### Generation parameters\n",
        "\n",
        "\n",
        "* `seed`: This variable is used to set a specific random seed for reproducibility.\n",
        "* `guidance_scale`: This parameter controls the extent to which the model should follow the prompt in text-to-image generation tasks, with higher values leading to stronger adherence to the prompt.       \n",
        "* `num_inference_steps`: This specifies the number of steps the model takes to generate an image. More steps can lead to a higher quality image but take longer to generate.        \n",
        "* `num_interpolation_steps`: This determines the number of steps used when interpolating between two points in the latent space, affecting the smoothness of transitions in generated       animations.        \n",
        "* `height`: The height of the generated images in pixels.       \n",
        "* `width`: The width of the generated images in pixels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTFrAlwrDiOI"
      },
      "source": [
        "### Interpolation between multiple prompts\n",
        "\n",
        "In contrast to the first example, where we moved away from a single prompt, in this example, we will be interpolating between any number of prompts. To do so, we will take consecutive pairs of prompts and create smooth transitions between them. Then, we will combine the interpolations of these consecutive pairs, and instruct the model to generate images based on them. For interpolation we will use the slerp function, as in the second example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QFtzMlOCAzd"
      },
      "source": [
        "![Example 3](https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/sd_interpolation_3.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "j1ZSbG2v2hUN"
      },
      "outputs": [],
      "source": [
        "# @markdown Specify a list of prompts (a prompt per line)\n",
        "\n",
        "from ipywidgets import Textarea, HBox\n",
        "\n",
        "try:\n",
        "  value=list1.value\n",
        "except:\n",
        "  value=None\n",
        "\n",
        "list1 = Textarea(\n",
        "    value=value,\n",
        "    layout={'width': '45%', 'height':'95%'},\n",
        "    description='Prompts')\n",
        "\n",
        "try:\n",
        "  value=list2.value\n",
        "except:\n",
        "  value=None\n",
        "\n",
        "list2 = Textarea(\n",
        "    value=value,\n",
        "    layout={'width': '45%', 'height':'95%'},\n",
        "    description='Negative<br/>prompts')\n",
        "\n",
        "display(HBox([list1,list2], layout={'height':'250px'}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "e_NEQjG_CAze"
      },
      "outputs": [],
      "source": [
        "seed = -1 # @param {type:\"integer\"}\n",
        "guidance_scale = 8 # @param {type:\"slider\", min:0, max:20, step:0.1}\n",
        "num_inference_steps = 10 # @param {type:\"slider\", min:0, max:50, step:1}\n",
        "num_interpolation_steps = 10 # @param {type:\"slider\", min:1, max:100, step:1}\n",
        "height = 512 # @param {type:\"slider\", min:256, max:1024, step:1}\n",
        "width = 512 # @param {type:\"slider\", min:256, max:1024, step:1}\n",
        "\n",
        "if seed >= 0:\n",
        "    generator = torch.manual_seed(seed)\n",
        "    print( \"seed:\", seed)\n",
        "else:\n",
        "    generator = None\n",
        "    print( \"seed:\", torch.seed() )\n",
        "\n",
        "\n",
        "# Once again, let's tokenize and obtain embeddings but this time for multiple positive and negative text prompts.\n",
        "\n",
        "prompts = list1.value.split('\\n')\n",
        "negative_prompts = list2.value.split('\\n')\n",
        "\n",
        "batch_size = len(prompts)\n",
        "\n",
        "if len(negative_prompts) < batch_size:\n",
        "  negative_prompts += [''] * (batch_size - len(negative_prompts))\n",
        "else:\n",
        "  negative_prompts = negative_prompts[:batch_size]\n",
        "\n",
        "# Tokenizing and encoding prompts into embeddings.\n",
        "prompts_tokens = pipe.tokenizer(\n",
        "    prompts,\n",
        "    padding=\"max_length\",\n",
        "    max_length=pipe.tokenizer.model_max_length,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "prompts_embeds = pipe.text_encoder(\n",
        "    prompts_tokens.input_ids.to(device)\n",
        ")[0]\n",
        "\n",
        "negative_prompts_tokens = pipe.tokenizer(\n",
        "    negative_prompts,\n",
        "    padding=\"max_length\",\n",
        "    max_length=pipe.tokenizer.model_max_length,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "negative_prompts_embeds = pipe.text_encoder(\n",
        "    negative_prompts_tokens.input_ids.to(device)\n",
        ")[0]\n",
        "\n",
        "\n",
        "# We will take consecutive pairs of prompts and create smooth transitions between them with `slerp` function.\n",
        "\n",
        "# Generating initial U-Net latent vectors from a random normal distribution.\n",
        "latents = torch.randn(\n",
        "    (1, pipe.unet.config.in_channels, height // 8, width // 8),\n",
        "    generator=generator,\n",
        ")\n",
        "\n",
        "# Interpolating between embeddings pairs for the given number of interpolation steps.\n",
        "interpolated_prompt_embeds = []\n",
        "interpolated_negative_prompts_embeds = []\n",
        "for i in range(batch_size - 1):\n",
        "    interpolated_prompt_embeds.append(\n",
        "        slerp(\n",
        "            prompts_embeds[i],\n",
        "            prompts_embeds[i + 1],\n",
        "            num_interpolation_steps\n",
        "        )\n",
        "    )\n",
        "    interpolated_negative_prompts_embeds.append(\n",
        "        slerp(\n",
        "            negative_prompts_embeds[i],\n",
        "            negative_prompts_embeds[i + 1],\n",
        "            num_interpolation_steps,\n",
        "        )\n",
        "    )\n",
        "\n",
        "interpolated_prompt_embeds = torch.cat(\n",
        "    interpolated_prompt_embeds, dim=0\n",
        ").to(device)\n",
        "\n",
        "interpolated_negative_prompts_embeds = torch.cat(\n",
        "    interpolated_negative_prompts_embeds, dim=0\n",
        ").to(device)\n",
        "\n",
        "\n",
        "# Finally, we need to generate images based on the embeddings.\n",
        "\n",
        "# Generating images using the interpolated embeddings.\n",
        "images = []\n",
        "for prompt_embeds, negative_prompt_embeds in tqdm(\n",
        "    zip(interpolated_prompt_embeds, interpolated_negative_prompts_embeds),\n",
        "    total=len(interpolated_prompt_embeds),\n",
        "):\n",
        "    images.append(\n",
        "        pipe(\n",
        "            height=height,\n",
        "            width=width,\n",
        "            num_images_per_prompt=1,\n",
        "            prompt_embeds=prompt_embeds[None, ...],\n",
        "            negative_prompt_embeds=negative_prompt_embeds[None, ...],\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            generator=generator,\n",
        "            latents=latents,\n",
        "        ).images\n",
        "    )\n",
        "\n",
        "# Display of saved generated images.\n",
        "path = save_images(images, save_path)\n",
        "display_gif(path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tgbkOxbkf8B"
      },
      "source": [
        "# Finalizing\n",
        "\n",
        "When you finish working you have to remember to **stop the runtime**, because there is a time limit and to avoid wasting resources. To stop the runtime click Manage Sessions on the Runtime menu. Once the dialog opens click terminate on the current runtime.\n",
        "\n",
        "> But when you stop the runtime everything you have not saved is ⚠ **lost** ⚠, so be sure to **download** everything you want to keep before stopping it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNi8VV-wJDFi"
      },
      "source": [
        "# Credits\n",
        "\n",
        "Modified from https://huggingface.co/learn/cookbook/en/stable_diffusion_interpolation by Taller Estampa https://tallerestampa.com / https://github.com/estampa"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30648,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}