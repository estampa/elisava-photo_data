{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv1yUcSgd27m"
      },
      "source": [
        "# Running a neural network using a library\n",
        "\n",
        "This continues what we did in the first notebook because we will continue using [Huggingface Transformers](https://github.com/huggingface/transformers). We could also use  [Huggingface Diffusers](https://github.com/huggingface/diffusers) that work with image generation and uses the same workflow.\n",
        "\n",
        "We will try to understand more the code that we used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we install the library."
      ],
      "metadata": {
        "id": "yxqDfdomsA6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!pip install -q transformers gradio ultralytics"
      ],
      "metadata": {
        "id": "SvOI4TDZJMKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipelines"
      ],
      "metadata": {
        "id": "0Cv2G1iQMoEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This library can run different tasks, that they organize in what they call [pipelines](https://huggingface.co/docs/transformers/pipeline_tutorial), and they are based on tasks or input/output format, like DepthEstimation or TextToAudio ([list](https://huggingface.co/docs/transformers/main_classes/pipelines)).\n",
        "\n",
        "We can create a pipeline with the task name\n",
        "\n",
        "```\n",
        "transcriber = pipeline(task=\"automatic-speech-recognition\")\n",
        "```\n",
        "or by selecting the model\n",
        "```\n",
        "transcriber = pipeline(model=\"openai/whisper-large-v2\")\n",
        "```"
      ],
      "metadata": {
        "id": "hI1KV6BjKhkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "captioner = pipeline(model=\"Salesforce/blip-image-captioning-base\", device=0)"
      ],
      "metadata": {
        "id": "93AW3QCgKg3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once created we can give it an input and it will process it and give us an output."
      ],
      "metadata": {
        "id": "TQwX9i8-MJAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = captioner(\"https://farm4.staticflickr.com/3129/3189318645_5466feb31a_z.jpg\")"
      ],
      "metadata": {
        "id": "AQTL7AAoNAq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4pnQh9yMTre"
      },
      "source": [
        "The output format varies with the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "id": "StkjTo2jM9zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = captioner(\"https://farm4.staticflickr.com/3129/3189318645_5466feb31a_z.jpg\", max_new_tokens=8)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "MSVtWrRjNYm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models\n",
        "\n",
        "But we can also search for a specific model and use it directly without a piepline. The code is more complex, but we can have more control.\n",
        "\n",
        "We can search for a model here https://huggingface.co/models and usually they provide the code needed.\n",
        "\n",
        "In this example we are going to use the DPT model for Depth Estimation https://huggingface.co/Intel/dpt-large.\n"
      ],
      "metadata": {
        "id": "AjPq66_8MwwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title â–¶ Use the DPT model for depth estimation\n",
        "\n",
        "#@markdown `processor = DPTImageProcessor.from_pretrained(\"Intel/dpt-hybrid-midas\")`\n",
        "\n",
        "#@markdown `model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\")`\n",
        "\n",
        "#@markdown This cell has the code hidden, double click to view it.\n",
        "\n",
        "from transformers import DPTImageProcessor, DPTForDepthEstimation\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = \"https://farm4.staticflickr.com/3129/3189318645_5466feb31a_z.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "processor = DPTImageProcessor.from_pretrained(\"Intel/dpt-hybrid-midas\")\n",
        "model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\")\n",
        "model.to('cuda')\n",
        "\n",
        "def process_depth(image):\n",
        "  # prepare image for the model\n",
        "  inputs = processor(images=image, return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = model(**inputs)\n",
        "      predicted_depth = outputs.predicted_depth\n",
        "\n",
        "  # interpolate to original size\n",
        "  prediction = torch.nn.functional.interpolate(\n",
        "      predicted_depth.unsqueeze(1),\n",
        "      size=image.size[::-1],\n",
        "      mode=\"bicubic\",\n",
        "      align_corners=False,\n",
        "  )\n",
        "\n",
        "  # visualize the prediction\n",
        "  output = prediction.squeeze().cpu().numpy()\n",
        "  formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
        "\n",
        "  return formatted, \"\"\n",
        "\n",
        "depth, _ = process_depth(image)\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "CNvHVZG8N72O",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the ouput"
      ],
      "metadata": {
        "id": "Se_2e-w0QUv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(depth)"
      ],
      "metadata": {
        "id": "qXUiwl5KPpte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PBOMwsyfwnb"
      },
      "source": [
        "## Initialize another library (Yolo) for other computer vision tasks\n",
        "\n",
        "First we load the models.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "yolo_detection = YOLO('yolov8m.pt')  # load a model for object detection\n",
        "yolo_pose = YOLO('yolov8m-pose.pt')  # load a model for pose detection\n",
        "yolo_seg = YOLO('yolov8m-seg.pt')    # load a model for segmentation"
      ],
      "metadata": {
        "id": "rPSdxyxAgNu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we define the function to process the images"
      ],
      "metadata": {
        "id": "dBhrjeuHxwAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_detections(image):\n",
        "  \"\"\"\n",
        "  This function applies object detection to the image\n",
        "\n",
        "  Args:\n",
        "    image (PIL.Image): The input image.\n",
        "\n",
        "  Returns:\n",
        "    tuple: A tuple containing the image with detections plotted and the JSON representation of the detections.\n",
        "  \"\"\"\n",
        "  detections = yolo_detection(image)\n",
        "  detection_image = detections[0].plot(img=Image.new('RGB', (detections[0].orig_shape[1], detections[0].orig_shape[0])))\n",
        "  return detection_image, detections[0].tojson()\n",
        "\n",
        "def process_pose(image):\n",
        "  \"\"\"\n",
        "  This function applies pose detection to the image\n",
        "\n",
        "  Args:\n",
        "    image (PIL.Image): The input image.\n",
        "\n",
        "  Returns:\n",
        "    tuple: A tuple containing the image with pose plotted and the JSON representation of the pose.\n",
        "  \"\"\"\n",
        "  pose = yolo_pose(image)\n",
        "  pose_image = pose[0].plot(boxes=False, labels=False, img=Image.new('RGB', (pose[0].orig_shape[1], pose[0].orig_shape[0])))\n",
        "  return pose_image, pose[0].tojson()\n",
        "\n",
        "def process_seg(image):\n",
        "  \"\"\"\n",
        "  This function applies segmentation to the image\n",
        "\n",
        "  Args:\n",
        "    image (PIL.Image): The input image.\n",
        "\n",
        "  Returns:\n",
        "    tuple: A tuple containing the image with segmentation plotted and the JSON representation of the segmentation.\n",
        "  \"\"\"\n",
        "  seg = yolo_seg(image)\n",
        "  seg_image = seg[0].plot(boxes=False, labels=False, img=Image.new('RGB', (seg[0].orig_shape[1], seg[0].orig_shape[0])))\n",
        "  return seg_image, seg[0].tojson()"
      ],
      "metadata": {
        "id": "4pZfeMetxvLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process an image with the different models"
      ],
      "metadata": {
        "id": "0HMhmUiFriH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Load an image\n",
        "url = \"https://farm4.staticflickr.com/3129/3189318645_5466feb31a_z.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "# Apply object detection\n",
        "detection_image, detection_json = process_detections(image)\n",
        "display(detection_image)\n",
        "\n",
        "# Apply pose detection\n",
        "pose_image, pose_json = process_pose(image)\n",
        "display(pose_image)\n",
        "\n",
        "# Apply segmentation\n",
        "seg_image, seg_json = process_seg(image)\n",
        "display(seg_image)"
      ],
      "metadata": {
        "id": "kC43HcDChNH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THnqWirZNRUG"
      },
      "source": [
        "## Google drive\n",
        "\n",
        "Optional: connect to google drive if you want to use images from there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqmgiToaOQz3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Give it a GUI\n",
        "\n",
        "We can also create a GUI in Colab.\n"
      ],
      "metadata": {
        "id": "1GWqW84CFcEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown  â–¶  We first define some processing functions\n",
        "\n",
        "#@markdown This cell has the code hidden, double click to view it.\n",
        "\n",
        "from PIL import Image, ImageOps\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "\n",
        "def process_caption(image):\n",
        "  output = captioner(image)\n",
        "  return None, output\n",
        "\n",
        "def process_single(image, task):\n",
        "  \"\"\"\n",
        "  This function processes the given image based on the specified task.\n",
        "\n",
        "  Args:\n",
        "    image (PIL.Image): The input image.\n",
        "    task (str): The task to be performed. It can be \"detection\", \"segmentation\", \"pose\", \"depth\" or \"caption\".\n",
        "\n",
        "  Returns:\n",
        "    tuple: A tuple containing the image with the task results plotted and the JSON representation of the results.\n",
        "    If the task is not recognized, it defaults to processing depth.\n",
        "  \"\"\"\n",
        "  if task == \"detection\":\n",
        "    return process_detections(image)\n",
        "  elif task == \"segmentation\":\n",
        "    return process_seg(image)\n",
        "  elif task == \"pose\":\n",
        "    return process_pose(image)\n",
        "  elif task == \"depth\":\n",
        "    return process_depth(image)\n",
        "  elif task == \"caption\":\n",
        "    return process_caption(image)\n",
        "\n",
        "def process(uploaded_files, local_files, output_folder, task):\n",
        "    \"\"\"\n",
        "    This function processes the images selected in the GUI\n",
        "\n",
        "    Args:\n",
        "      uploaded_files: files uploaded through the GUI\n",
        "      local_files: files selected from the local file system\n",
        "      output_folder: the folder where the results will be saved\n",
        "      task (str): The task to be performed. It can be \"detection\", \"segmentation\", \"pose\", or \"depth\".\n",
        "\n",
        "    Returns:\n",
        "      tuple: A tuple of lists containing the processed images and the corresponding JSON files\n",
        "    \"\"\"\n",
        "\n",
        "    # Check the input\n",
        "    if len(output_folder) == 0:\n",
        "        raise gr.Error(\"You have to select an output folder!\")\n",
        "\n",
        "    if uploaded_files is None and len(local_files) == 0:\n",
        "        raise gr.Error(\"You have to select at least one file or folder!\")\n",
        "\n",
        "    output_folder = os.path.dirname(output_folder[0]) if not os.path.isdir(output_folder[0]) else output_folder[0]\n",
        "\n",
        "    input_files = []\n",
        "    output_images = []\n",
        "    output_json = []\n",
        "\n",
        "    # List all the files to be processed\n",
        "    if uploaded_files is not None:\n",
        "        if isinstance(uploaded_files, list):\n",
        "            input_files.extend(uploaded_files)\n",
        "        else:\n",
        "            input_files.append(uploaded_files)\n",
        "\n",
        "    input_files.extend(local_files)\n",
        "\n",
        "    input_files = [input_file for input_file in input_files if input_file.lower().endswith(\".jpg\") and not input_file.lower().endswith(\".png\") and not os.path.isdir(input_file)]\n",
        "\n",
        "    # Process all the files\n",
        "    if len(input_files) > 0:\n",
        "        for input_file in input_files:\n",
        "            # Open the image\n",
        "            image = Image.open(input_file)\n",
        "            image = ImageOps.exif_transpose(image)\n",
        "\n",
        "            # Process the image\n",
        "            result_image, result_json = process_single(image, task)\n",
        "            output_json.append(result_json)\n",
        "\n",
        "            # Save the resulting image\n",
        "            if result_image is not None:\n",
        "              output_images.append(result_image)\n",
        "              output_filename = f\"{Path(input_file).stem}-{task}.png\"\n",
        "              output_path = os.path.join(output_folder, output_filename)\n",
        "              Image.fromarray(result_image).save(output_path)\n",
        "\n",
        "            yield output_images, output_json"
      ],
      "metadata": {
        "cellView": "form",
        "id": "n-WCKHGey3qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown  â–¶  Then we create the GUI\n",
        "\n",
        "import os\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "  with gr.Column():\n",
        "    input_file = gr.File(file_count=\"multiple\", file_types=[\".jpg\", \".png\"], label=\"Input images\")\n",
        "    input_files = gr.FileExplorer(label=\"Remote files\")\n",
        "    output_folder = gr.FileExplorer(label=\"Remote output folder\")\n",
        "    task = gr.Radio([\"detection\", \"segmentation\", \"pose\", \"depth\", \"caption\"], label=\"Task\", value=\"detection\")\n",
        "    process_button = gr.Button(value=\"Process\")\n",
        "\n",
        "  with gr.Column():\n",
        "    with gr.Tabs():\n",
        "      with gr.Tab(label=\"Images\"):\n",
        "        gallery = gr.Gallery(label=\"Processed images\", show_label=False, columns=[3], object_fit=\"contain\", height=\"auto\")\n",
        "      with gr.Tab(label=\"Data\"):\n",
        "        json = gr.JSON(label=\"Output data\")\n",
        "\n",
        "  process_button.click(process, [input_file, input_files, output_folder, task], [gallery, json])\n",
        "\n",
        "demo.launch(quiet=True, debug=False, height=768)\n"
      ],
      "metadata": {
        "id": "pjfFvGfWFvC5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tgbkOxbkf8B"
      },
      "source": [
        "# Finalizing\n",
        "\n",
        "When you finish working you have to remember to **stop the runtime**, because there is a time limit and to avoid wasting resources. To stop the runtime click Manage Sessions on the Runtime menu. Once the dialog opens click terminate on the current runtime.\n",
        "\n",
        "> But when you stop the runtime everything you have not saved is âš  **lost** âš , so be sure to **download** everything you want to keep before stopping it.\n"
      ]
    }
  ]
}